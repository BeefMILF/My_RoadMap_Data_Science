{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from dask_ml.preprocessing import StandardScaler\n",
    "    import gc\n",
    "    import time\n",
    "    import dask.dataframe as dd\n",
    "    from dask.distributed import Client, progress # see the sys load\n",
    "    \n",
    "except: ImportError(), 'Some modules have not loaded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set our files path\n",
    "test = 'test.tsv'\n",
    "train = 'train.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>level_2</th>\n",
       "      <th>level_3</th>\n",
       "      <th>level_4</th>\n",
       "      <th>level_5</th>\n",
       "      <th>level_6</th>\n",
       "      <th>level_7</th>\n",
       "      <th>level_8</th>\n",
       "      <th>level_9</th>\n",
       "      <th>...</th>\n",
       "      <th>level_247</th>\n",
       "      <th>level_248</th>\n",
       "      <th>level_249</th>\n",
       "      <th>level_250</th>\n",
       "      <th>level_251</th>\n",
       "      <th>level_252</th>\n",
       "      <th>level_253</th>\n",
       "      <th>level_254</th>\n",
       "      <th>level_255</th>\n",
       "      <th>id_job\\tfeatures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1864791934054678713\\t2</td>\n",
       "      <td>9835</td>\n",
       "      <td>9999</td>\n",
       "      <td>9941</td>\n",
       "      <td>9945</td>\n",
       "      <td>9386</td>\n",
       "      <td>9899</td>\n",
       "      <td>9421</td>\n",
       "      <td>9954</td>\n",
       "      <td>9952</td>\n",
       "      <td>...</td>\n",
       "      <td>8818</td>\n",
       "      <td>9954</td>\n",
       "      <td>9925</td>\n",
       "      <td>9934</td>\n",
       "      <td>8689</td>\n",
       "      <td>9958</td>\n",
       "      <td>9086</td>\n",
       "      <td>9114</td>\n",
       "      <td>9950</td>\n",
       "      <td>9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7413918695841089440\\t2</td>\n",
       "      <td>9082</td>\n",
       "      <td>9999</td>\n",
       "      <td>9700</td>\n",
       "      <td>9669</td>\n",
       "      <td>9981</td>\n",
       "      <td>9729</td>\n",
       "      <td>9822</td>\n",
       "      <td>9667</td>\n",
       "      <td>9526</td>\n",
       "      <td>...</td>\n",
       "      <td>9979</td>\n",
       "      <td>9752</td>\n",
       "      <td>9695</td>\n",
       "      <td>9676</td>\n",
       "      <td>9974</td>\n",
       "      <td>9788</td>\n",
       "      <td>9955</td>\n",
       "      <td>9907</td>\n",
       "      <td>9747</td>\n",
       "      <td>9824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   level_0  level_1  level_2  level_3  level_4  level_5  \\\n",
       "0   1864791934054678713\\t2     9835     9999     9941     9945     9386   \n",
       "1  -7413918695841089440\\t2     9082     9999     9700     9669     9981   \n",
       "\n",
       "   level_6  level_7  level_8  level_9  ...  level_247  level_248  level_249  \\\n",
       "0     9899     9421     9954     9952  ...       8818       9954       9925   \n",
       "1     9729     9822     9667     9526  ...       9979       9752       9695   \n",
       "\n",
       "   level_250  level_251  level_252  level_253  level_254  level_255  \\\n",
       "0       9934       8689       9958       9086       9114       9950   \n",
       "1       9676       9974       9788       9955       9907       9747   \n",
       "\n",
       "   id_job\\tfeatures  \n",
       "0              9875  \n",
       "1              9824  \n",
       "\n",
       "[2 rows x 257 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LoadBigCsvFile:\n",
    "    '''load data from tsv, transform, scale, add two columns'''\n",
    "    def __init__(self, train, test, scaler=StandardScaler(copy=False)):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.scaler = scaler\n",
    "    def read_data(self):\n",
    "        # use dask and read with c\n",
    "        try:\n",
    "            data_train = dd.read_csv(self.train, \\\n",
    "                                     dtype={n:'int16' for n in range(1, 300)}, engine='c').reset_index()\n",
    "            data_test = dd.read_csv(self.test, \\\n",
    "                                    dtype={n:'int16' for n in range(1, 300)}, engine='c').reset_index()\n",
    "        except: (IOError, OSError), 'can not open file'\n",
    "            \n",
    "#         assert data_train.isna()# no NaN\n",
    "#         assert train.shape[1] == test.shape[1] # check shape of features\n",
    "        assert len(data_test) != 0 and len(data_train) != 0 #if any data?\n",
    "        # fit and transform\n",
    "        self.scaler.fit(data_train.iloc[:,1:])\n",
    "        test_transformed = self.scaler.transform(data_test.iloc[:,1:])\n",
    "        # compute and add columns\n",
    "        test_transformed['max_feature_2_abs_mean_diff'] = abs(test_transformed.mean(axis=1) - test_transformed.max(axis=1))\n",
    "        test_transformed['max_feature_2_index'] = test_transformed.idxmin(axis=1)\n",
    "        test_transformed['job_id'] = data_test.iloc[:,0]\n",
    "        \n",
    "        return data_train\n",
    "    \n",
    "data = LoadBigCsvFile(train, test, scaler=StandardScaler(copy=False)).read_data()\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 257 entries, level_0 to id_job\tfeatures\n",
      "dtypes: object(1), int16(1), int64(255)"
     ]
    }
   ],
   "source": [
    "data.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# init class \n",
    "start_time = time.time()\n",
    "data_ = LoadBigCsvFile(train, test).read_data()\n",
    "gc.collect()\n",
    "print('class loaded in %s seconds' % (time.time() - start_time)) \n",
    "\n",
    "time.sleep(1) # set some time gap\n",
    "\n",
    "# save to hdf for later use or modification\n",
    "start_time = time.time()\n",
    "data_.to_hdf('test_transformed.hdf',  key='df1')\n",
    "print('file saved in hdf in %s seconds' % (time.time() - start_time))\n",
    "\n",
    "time.sleep(1) # set some time gap\n",
    "\n",
    "# check the file and its content \n",
    "start_time = time.time()\n",
    "hdf_read = dd.read_hdf('test_transformed.hdf', key='df1', mode='r', chunksize=10000)\n",
    "print('file load into system in %s seconds' % (time.time() - start_time))\n",
    "\n",
    "hdf_read.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadBigCsvFile:\n",
    "    \n",
    "    '''load data from tsv, transform, scale, add two columns'''\n",
    "    \n",
    "    def __init__(self, train, test, scaler=StandardScaler(copy=False)):\n",
    "\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.scaler = scaler\n",
    "    \n",
    "    def read_data(self):\n",
    "        # use dask for read with C \n",
    "        try:\n",
    "            data_train = dd.read_csv(self.train, sep=',', header=None, skiprows=1, dtype={n:'int16' for n in range(1, 300)}, engine='c')\n",
    "            data_test = dd.read_csv(self.test, sep=',', header=None, skiprows=1, dtype={n:'int16' for n in range(1, 300)}, engine='c')\n",
    "            \n",
    "        except: MemoryError, 'not enough memory'\n",
    "            \n",
    "        assert train.isna().sum().sum() == 0 # no NaN\n",
    "        assert train.shape[1] == test.shape[1] # check shape of features\n",
    "        assert len(train) != 0 and len(test) != 0 #if any data?\n",
    "        \n",
    "        # fit from train and scale to test\n",
    "        self.scaler.fit(data_train.iloc[:,1:])\n",
    "        temp = self.scaler.transform(data_test.iloc[:,1:])\n",
    "        \n",
    "        del data_train # del not needed\n",
    "        \n",
    "        # index of max element\n",
    "        temp['max_feature_2_index'] = temp.idxmax(axis=1)\n",
    "        \n",
    "        # calculate absolute diviation from max value in row\n",
    "        temp['max_feature_2_abs_mean_diff'] = abs(temp.max(axis=1) - temp.mean(axis=1))\n",
    "        \n",
    "        # set id columnsD\n",
    "        temp['job_id'] = data_test.iloc[:,0] \n",
    "        \n",
    "        del data_test # del not needed\n",
    "        \n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with 5G file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set workers\n",
    "client = Client(n_workers=2, threads_per_worker=2, memory_limit='3GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the number of rows for the CSV file\n",
    "start_time = time.time()\n",
    "\n",
    "N = 5_000_000\n",
    "columns = 257\n",
    "\n",
    "# create DF \n",
    "df = pd.DataFrame(np.random.randint(999, 999999, size=(N, columns)), columns=['level_%s' % i for i in range(0, columns)])\n",
    "\n",
    "print('%s seconds' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df to csv \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df.to_csv('random.csv', sep=',')\n",
    "\n",
    "print('%s seconds' % (time.time() - start_time)) # 877.5422155857086 seconds, 10 G!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gib = 'jobble_data/random.csv' # 5 mln records! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadBigCsvFile_TRAIN:\n",
    "    \n",
    "    '''load data from tsv, transform, scale, add two columns'''\n",
    "    \n",
    "    def __init__(self, train, scaler=StandardScaler(copy=False)):\n",
    "\n",
    "        self.train = train\n",
    "        self.scaler = scaler\n",
    "    \n",
    "    def read_data(self):\n",
    "        # use dask for read with C \n",
    "        try:\n",
    "            data_train = dd.read_csv(self.train, sep=',', header=None, skiprows=1, dtype={n:'int16' for n in range(1, 300)}, engine='c')            \n",
    "        except: MemoryError, 'not enough memory'\n",
    "        \n",
    "        # fit from train and scale to test\n",
    "        self.scaler.fit(data_train.iloc[:,1:])\n",
    "        temp = self.scaler.transform(data_train.iloc[:,1:])\n",
    "                \n",
    "        # index of max element\n",
    "        temp['max_feature_2_index'] = temp.idxmax(axis=1)\n",
    "        \n",
    "        # calculate absolute diviation from max value in row\n",
    "        temp['max_feature_2_abs_mean_diff'] = abs(temp.max(axis=1) - temp.mean(axis=1))\n",
    "        \n",
    "        # set id columnsD\n",
    "        temp['job_id'] = data_train.iloc[:,0] \n",
    "        \n",
    "        del data_train # del not needed\n",
    "        \n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''init the class,'''\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rer = LoadBigCsvFile_TRAIN(gib, StandardScaler(copy=False)).read_data()\n",
    "\n",
    "print('%s seconds' % (time.time() - start_time)) # 10 ces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to HDF\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rer.to_hdf('random.hdf',  key='df1')\n",
    "\n",
    "print('%s seconds' % (time.time() - start_time)) #152.77526926994324 seconds!!! 10.4 G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from HDF\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "hdf_read = dd.read_hdf('random.hdf', key='df1', mode='r', chunksize=10000)\n",
    "\n",
    "print('%s seconds' % (time.time() - start_time)) # 0.03516435623168945 seconds!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf_read.head(3) # read head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(hdf_read) == 5_000_000 # test 5ml records"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
